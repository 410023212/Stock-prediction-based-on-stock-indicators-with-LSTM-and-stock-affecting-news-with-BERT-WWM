{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm,tqdm_notebook, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, multilabel_confusion_matrix\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset, Subset)\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Line_Regression(nn.Module):\n",
    "    def __init__(self,basic_input_size, dynamic_input_size, output_size=2, hidden_size = 32, num_layers=3):\n",
    "        super(LSTM_Line_Regression,self).__init__()\n",
    "        \n",
    "        self.basic_input_size = basic_input_size\n",
    "        self.basic_output_size = 32\n",
    "        \n",
    "        \n",
    "        self.dynamic_input_size = dynamic_input_size\n",
    "        self.dynamic_output_size = hidden_size\n",
    "        self.drop_prob = 0.4\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.concate_size = self.dynamic_output_size + self.basic_output_size\n",
    "        self.final_hidden_size = 16\n",
    "        \n",
    "        self.line_basic = nn.Sequential(\n",
    "             nn.Linear(self.basic_input_size, self.basic_output_size),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(p=self.drop_prob),\n",
    "             nn.Linear(self.basic_output_size, self.basic_output_size))\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(dynamic_input_size, hidden_size, num_layers, batch_first=True, dropout=0.4)\n",
    "        \n",
    "        self.line_1 = nn.Linear(self.concate_size, self.final_hidden_size)\n",
    "        #nn.init.kaiming_normal_(self.line_1.weight, mode='fan_out')\n",
    "        self.line_final = nn.Linear(self.final_hidden_size, output_size)\n",
    "        #nn.init.kaiming_normal_(self.line_final.weight, mode='fan_out')\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, basic, dynamic):\n",
    "        # dynamic shape (batch, seq_len , input_size)\n",
    "        \n",
    "        # dynamic\n",
    "        d_out, (_, _) = self.lstm(dynamic)  \n",
    "        d_out = d_out[:, -1, :]\n",
    "        \n",
    "        # basic\n",
    "        b_out = self.line_basic(basic)\n",
    "        \n",
    "        # concate\n",
    "        cat = torch.cat((b_out, d_out), 1)\n",
    "        out = F.relu(cat)\n",
    "        \n",
    "        out = F.relu(self.dropout(self.line_1(out)))\n",
    "        out = self.line_final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Line_Regression_2(nn.Module):\n",
    "\n",
    "    def __init__(self,basic_input_size, dynamic_input_size, output_size=1, hidden_size = 32, num_layers=3):\n",
    "        super(LSTM_Line_Regression_2,self).__init__()\n",
    "        self.basic_input_size = basic_input_size\n",
    "        self.basic_output_size = 32\n",
    "        \n",
    "        self.dynamic_input_size = dynamic_input_size\n",
    "        self.dynamic_output_size = hidden_size\n",
    "        self.drop_prob = 0.4\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.concate_size = self.dynamic_output_size + self.basic_output_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(dynamic_input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.line_1 = nn.Linear(self.concate_size, 32)\n",
    "        nn.init.kaiming_normal_(self.line_1.weight, mode='fan_out')\n",
    "        self.line_final = nn.Linear(self.concate_size, 1)\n",
    "        #nn.init.kaiming_normal_(self.line_final.weight, mode='fan_out')\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.line_basic = nn.Sequential(\n",
    "             nn.Linear(self.basic_input_size, self.basic_output_size),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(p=self.drop_prob),\n",
    "             nn.Linear(self.basic_output_size, self.basic_output_size))\n",
    "        \n",
    "    def forward(self, basic, dynamic):\n",
    "        # dynamic shape (batch, seq_len , input_size)\n",
    "        \n",
    "        # dynamic\n",
    "        d_out, (_, _) = self.lstm(dynamic)  \n",
    "        d_out = d_out[:, -1, :]\n",
    "        \n",
    "        # basic\n",
    "        # b_out = self.line_basic(basic)\n",
    "        \n",
    "        # concate\n",
    "        #out = torch.cat((b_out, d_out), 1)\n",
    "        out = F.relu(d_out)\n",
    "        \n",
    "        #out = F.relu(self.dropout(self.line_1(out)))\n",
    "        out = self.line_final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 model=LSTM_Line_Regression(5, 5),\n",
    "                 train_batch_size=128,\n",
    "                 data_class=None,\n",
    "                 device=torch.device(\"cpu\"),\n",
    "                 train_dataset=None,\n",
    "                 dev_dataset=None\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.device = device\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.default_setting()\n",
    "\n",
    "    def default_setting(self):\n",
    "\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # pw = torch.tensor([0.5]).to(torch.device(device))\n",
    "        # loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=pw) #\n",
    "\n",
    "        adam_epsilon = 1e-8\n",
    "        weight_decay = 1e-4\n",
    "        LEARNING_RATE = 1e-3\n",
    "        model = self.model\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        self.optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=adam_epsilon)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=1)\n",
    "\n",
    "        # store perfomation\n",
    "        self.min_dev_loss = float('inf')\n",
    "        self.best_acc = -float('inf')\n",
    "\n",
    "        self.train_results = []\n",
    "        self.dev_results = []\n",
    "        self.lr_list = []\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.loss_function.to(self.device)\n",
    "\n",
    "    def train_epoch(self, N_EPOCHS, fpath='./buffer.bin'):\n",
    "        for epoch in trange(N_EPOCHS):\n",
    "            # logger.info('_____ Epoch %s begin _____'% epoch)\n",
    "            res, preds, model_output = self.train_func(self.train_dataset, self.model)\n",
    "            self.train_results.append(res)\n",
    "\n",
    "            res, preds, model_output = self.dev_func(self.dev_dataset, self.model)\n",
    "            self.dev_results.append(res)\n",
    "\n",
    "            self.lr_list.append(self.optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "            dev_acc = res['acc']\n",
    "            if dev_acc > self.best_acc:\n",
    "                # tqdm.write(\"@@@@@ Save MAX_ACC model @@@@@\")\n",
    "                # logger.info('file_path = %S'% output_model_file)\n",
    "                self.best_acc = dev_acc\n",
    "                torch.save(self.model, fpath)\n",
    "                self.acc_model = torch.load(fpath)\n",
    "\n",
    "            dev_loss = res['loss']\n",
    "            if dev_loss < self.min_dev_loss:\n",
    "                # tqdm.write(\"@@@@@ Save MIN_LOSS model @@@@@\")\n",
    "                # logger.info('file_path = %S'% output_model_file)\n",
    "                self.min_dev_loss = dev_loss\n",
    "                torch.save(self.model, fpath)\n",
    "                self.loss_model = torch.load(fpath)\n",
    "\n",
    "        logger.info(\"_____ Train finish _____\")\n",
    "\n",
    "        return self.train_results, self.dev_results, preds\n",
    "\n",
    "    def save_model(self, output_model_file=None):\n",
    "        logger.info(\"@@@@@ Save best model @@@@@\")\n",
    "        logger.info('file_path = %S' % output_model_file)\n",
    "        torch.save(self.model, output_model_file)\n",
    "        # model.config.to_json_file(output_config_file)\n",
    "        # tokenizer.save_vocabulary(OUTPUT_DIR)\n",
    "        return True\n",
    "\n",
    "    def show_lr(self):\n",
    "        # plot LEARNING_RATE\n",
    "        plt.plot(range(len(self.lr_list)), self.lr_list, color='r')\n",
    "        plt.show()\n",
    "\n",
    "    def show_statistic(self):\n",
    "        train_results = self.train_results\n",
    "        dev_results = self.dev_results\n",
    "        data_class = self.data_class\n",
    "        logger.info(' TRAIN\\n' + str(get_class_ratio(data_class.t_all_output)))\n",
    "        logger.info(' DEV\\n' + str(get_class_ratio(data_class.d_all_output)))\n",
    "\n",
    "        show_map(train_results, dev_results, 'acc', data_class=data_class)\n",
    "        show_map(train_results, dev_results, 'loss')\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            plt_class(train_results, dev_results, data_class, i)\n",
    "\n",
    "    def show_precision(self, results):\n",
    "        logger.info('dev')\n",
    "        logger.info('acc = %.3f' % results[-1]['acc'])\n",
    "        for i in range(self.num_classes):\n",
    "            #     print('class %s mcm = \\n%s' % (i,dev_results[-1]['mcm'][i]))\n",
    "            logger.info('class %s Precision = %.3f' % (i, get_precision(results[-1]['mcm'][i].ravel())))\n",
    "\n",
    "    def train_func(self, sub_dataset, model):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        labels = []\n",
    "        preds = []\n",
    "        \n",
    "        mysampler = RandomSampler(sub_dataset)\n",
    "        data_loader = DataLoader(sub_dataset, sampler=mysampler, batch_size=self.train_batch_size)\n",
    "\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            # train \n",
    "            self.optimizer.zero_grad()\n",
    "            basic, dynamic, _y = batch\n",
    "            # print(basic)\n",
    "            # print(_y.shape)\n",
    "            out = model(basic, dynamic)\n",
    "            loss = self.loss_function(out, _y)\n",
    "            # print(out)\n",
    "            # print(loss.item())\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (out.argmax(1) == _y).sum().item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            labels += _y.cpu().numpy().tolist()\n",
    "            preds += out.argmax(1).cpu().numpy().tolist()\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "        # evaluate\n",
    "        result = {}\n",
    "        result = get_eval_report('train', labels, preds)\n",
    "        result['loss'] = train_loss / len(data_loader)\n",
    "        result['acc'] = train_acc / len(sub_dataset)\n",
    "\n",
    "        # print(preds)\n",
    "        model_output = out.data.cpu().numpy()\n",
    "        return result, preds, model_output\n",
    "\n",
    "    def dev_func(self, sub_dataset, model):\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        labels = []\n",
    "        preds = []\n",
    "\n",
    "        basic, dynamic, _y = sub_dataset\n",
    "        # dev\n",
    "        out = model(basic, dynamic)\n",
    "        loss = self.loss_function(out, _y)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += (out.argmax(1) == _y).sum().item()\n",
    "\n",
    "        # evaluate\n",
    "        result = {}\n",
    "        labels += _y.cpu().numpy().tolist()\n",
    "        preds += out.argmax(1).cpu().numpy().tolist()\n",
    "        result = get_eval_report('dev', labels, preds)\n",
    "        result['loss'] = total_loss / 1\n",
    "        result['acc'] = total_acc / len(_y)\n",
    "\n",
    "        # print(preds)\n",
    "        model_output = out.data.cpu().numpy()\n",
    "        return result, preds, model_output\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(sub_dataset,model):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    mysampler = RandomSampler(sub_dataset)\n",
    "    data_loader = DataLoader(sub_dataset, sampler=mysampler, batch_size=train_batch_size)\n",
    "    \n",
    "    \n",
    "    for step, batch in enumerate(data_loader):\n",
    "        # train \n",
    "        optimizer.zero_grad()\n",
    "        basic, dynamic, _y = batch\n",
    "        #print(basic)\n",
    "        #print(_y.shape)\n",
    "        out = model(basic, dynamic)\n",
    "        loss = loss_function(out, _y)\n",
    "        #print(out)\n",
    "        #print(loss.item())\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (out.argmax(1) == _y).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        labels += _y.cpu().numpy().tolist()\n",
    "        preds += out.argmax(1).cpu().numpy().tolist()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # evaluate\n",
    "    result = {}\n",
    "    result = get_eval_report('train', labels, preds)\n",
    "    result['loss'] = train_loss / len(data_loader)\n",
    "    result['acc'] = train_acc / len(sub_dataset)\n",
    "    \n",
    "    #print(preds)\n",
    "    model_output = out.data.cpu().numpy()\n",
    "    return result, preds, model_output\n",
    "\n",
    "def get_eval_report(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    # processor.get_labels()\n",
    "    mcm = multilabel_confusion_matrix(labels, preds, labels=list(range(2)))\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcm\": mcm\n",
    "    }\n",
    "\n",
    "def show_eval_report(result,name = ''):\n",
    "    logger.info(\"***** Eval %s results *****\" %name )\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        \n",
    "\n",
    "def dev_func(sub_dataset, model):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "     \n",
    "    basic, dynamic, _y = sub_dataset\n",
    "    # dev\n",
    "    out = model(basic, dynamic)\n",
    "    loss = loss_function(out, _y)\n",
    "    total_loss += loss.item()\n",
    "    total_acc += (out.argmax(1) == _y).sum().item()\n",
    "    \n",
    "    \n",
    "    # evaluate\n",
    "    result = {}\n",
    "    labels += _y.cpu().numpy().tolist()\n",
    "    preds += out.argmax(1).cpu().numpy().tolist()\n",
    "    result = get_eval_report('dev', labels, preds)\n",
    "    result['loss'] = total_loss / 1\n",
    "    result['acc'] = total_acc / len(_y)\n",
    "    \n",
    "    #print(preds)\n",
    "    model_output = out.data.cpu().numpy()\n",
    "    return result, preds, model_output\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_map(train_results, dev_results, indicator, t_all_output,d_all_output):\n",
    "    x = np.arange(0, len(train_results))\n",
    "    y_train = [res[indicator] for res in train_results]\n",
    "    y_dev = [res[indicator] for res in dev_results]\n",
    "    # plt.rcParams['figure.dpi'] = 300\n",
    "    plt.title(\"%s trend\" % indicator)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(indicator)\n",
    "    if indicator == 'acc':\n",
    "        plt.ylim((0, 1.0))\n",
    "        if True:\n",
    "            train_max_class_ratio = get_max_class_ratio(t_all_output)\n",
    "            dev_max_class_ratio = get_max_class_ratio(d_all_output)\n",
    "            plt.axhline(y=train_max_class_ratio, color='r', linewidth=0.5)\n",
    "            plt.axhline(y=dev_max_class_ratio, color='b', linewidth=0.5)\n",
    "\n",
    "    plt.plot(x, y_train, color='red', label='train')\n",
    "    plt.plot(x, y_dev, color='blue', label='dev')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_precision(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def get_recall(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def get_F1_score(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    P_precision = tp / (tp + fp)\n",
    "    P_recall = tp / (tp + fn)\n",
    "    return 2 * (P_precision * P_recall) / (P_precision + P_recall)\n",
    "\n",
    "\n",
    "eval_functions = {\n",
    "    'precision': get_precision,\n",
    "    'recall': get_recall,\n",
    "    'F1_score': get_F1_score\n",
    "}\n",
    "\n",
    "\n",
    "def plt_class(train_results, dev_results, t_all_output,d_all_output, k):\n",
    "    x = np.arange(0, len(train_results))\n",
    "    for name, func in eval_functions.items():\n",
    "        y_train = [func(res['mcm'][k].ravel()) for res in train_results]\n",
    "        y_dev = [func(res['mcm'][k].ravel()) for res in dev_results]\n",
    "        plt.title(f\"CLASS {k} {name}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(f'{name}')\n",
    "        plt.ylim((0, 1.0))\n",
    "        plt.plot(x, y_train, color='red', label='train')\n",
    "        plt.plot(x, y_dev, color='blue', label='dev')\n",
    "        if (name == 'precision'):\n",
    "            train_max_class_ratio = get_class_ratio(t_all_output)[k]\n",
    "            dev_max_class_ratio = get_class_ratio(d_all_output)[k]\n",
    "            plt.axhline(y=train_max_class_ratio, color='r', linewidth=1)\n",
    "            plt.axhline(y=dev_max_class_ratio, color='b', linewidth=1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_class_ratio(all_output):\n",
    "    b = pd.DataFrame(all_output)\n",
    "    a = b[0].value_counts(sort=False, normalize=True).sort_index()\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_max_class_ratio(all_output):\n",
    "    b = pd.DataFrame(all_output)\n",
    "    a = b[0].value_counts(sort=False, normalize=True).sort_index()\n",
    "    return a.max()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = LSTM_Line_Regression(5,5)\n",
    "    LSTM_Line_Regression_2(5,5)\n",
    "    \n",
    "    model_class = TrainModel(model=LSTM_Line_Regression,\n",
    "                             days_for_train = 10,\n",
    "                             train_batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_wy",
   "language": "python",
   "name": "env_wy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
