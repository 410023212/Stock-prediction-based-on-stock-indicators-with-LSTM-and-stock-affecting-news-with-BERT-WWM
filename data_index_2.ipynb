{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data.py\n",
    "Function:\n",
    "    some label functions\n",
    "Class:\n",
    "    GetData_index\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('./dataquery')\n",
    "import DataQuery.MysqlAPI as MysqlAPI\n",
    "import DataQuery.DataToolkit as DataToolkit \n",
    "import re\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm,tqdm_notebook, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, multilabel_confusion_matrix\n",
    "from matplotlib import pyplot as plt \n",
    "import pickle  \n",
    "import talib\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset, Subset)\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.45\n"
     ]
    }
   ],
   "source": [
    "import tushare as ts\n",
    "print(ts.__version__)\n",
    "# initial\n",
    "ts.set_token('dbf6d939f7f819c12cbb82bd46d03c5df31c7c42e2201d8451e6c576')\n",
    "pro = ts.pro_api()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.0 (default, Oct  9 2018, 10:31:47) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "# label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# label news according to markets data\n",
    "def label_direction_3(trend):\n",
    "    if abs(trend) <= 0.02:\n",
    "        return 1\n",
    "    return 2 if trend > 0 else 0\n",
    "\n",
    "def label_direction_wy(trend):\n",
    "    for index, i in enumerate([-0.005,0.005]):\n",
    "        if trend < i:\n",
    "            return index\n",
    "    return index+1\n",
    "\n",
    "def label_direction_5(trend):\n",
    "    for index, i in enumerate([-0.02,-0.005,0.005,0.02]):\n",
    "        if trend < i:\n",
    "            return index\n",
    "    #print(trend)\n",
    "    return index+1\n",
    "    \n",
    "def label_direction_2(trend):\n",
    "    return 0 if trend <= 0 else 1\n",
    "\n",
    "def label_swing(trend):\n",
    "    if abs(trend) <= 0.02:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_direction_5(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def show_label_distribution(self, data_df):\n",
    "    a = data_df['label'].value_counts(sort=False).sort_index()\n",
    "    max_class_ratio = a.max()/a.sum()\n",
    "    logger.info('%s\\n'%code + str(a) + '\\nmax_class = %s' % max_class_ratio)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(n):\n",
    "    # move data to GPU\n",
    "    device = torch.device(\"cuda:%d\"%n if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = 'cuda:5'\n",
    "    logger.info(device)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GetData_index():\n",
    "    \"\"\"\n",
    "    key func:\n",
    "        self.generate\n",
    "        output:\n",
    "            train_dataset, dev_dataset\n",
    "            type : TensorDataset\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 task_name='index_P',\n",
    "                 datapath='/home/wy506wd/data/',\n",
    "                 train_date=[\"2016-01-01\", \"2019-07-01\"],\n",
    "                 index_code_list=['000300.SH', '000905.SH', '000016.SH'],\n",
    "                 device=torch.device(\"cpu\"),\n",
    "                 days_for_train=15,\n",
    "                 train_proportion=0.8,\n",
    "                 fromtime='2009-10-01',\n",
    "                 endtime='2019-07-01',\n",
    "                 label_back_days=5,\n",
    "                 label_func=label_direction_2,\n",
    "                 normalization_parameters = {}\n",
    "                 ):\n",
    "\n",
    "        self.datapath = datapath\n",
    "        # self.model_dir = '/home/wy506wd/download/'\n",
    "        self.data_dir = self.datapath\n",
    "        self.task_name = task_name\n",
    "        self.fromtime = fromtime\n",
    "        self.endtime = endtime\n",
    "        self.index_code_list = index_code_list\n",
    "        self.device = device\n",
    "        self.days_for_train = days_for_train\n",
    "        self.train_proportion = train_proportion\n",
    "        self.label_back_days = label_back_days\n",
    "        self.label_func = label_func\n",
    "\n",
    "        self.dynamic_input_size = None\n",
    "        self.basic_input_size = None\n",
    "        self.output_size = None\n",
    "        self.features = {}\n",
    "        self.normalization_parameters = normalization_parameters\n",
    "\n",
    "    def generate(self):\n",
    "        data_dict = {}\n",
    "        for index_code in self.index_code_list:\n",
    "            logger.info(index_code)\n",
    "            index_data = self.fetch_data(index_code)\n",
    "            index_data = self._calculate_features(index_data)\n",
    "            index_data = self.set_label(index_data, self.label_func, self.label_back_days)\n",
    "            index_data = self._normalization(index_data)\n",
    "            index_data = self.get_tushare_data(index_code, index_data)\n",
    "            # basic_data = self.get_basic_data(index_data)\n",
    "            # tecnical_data = self.get_technical_data(index_data)\n",
    "            # save\n",
    "            data_dict[index_code] = index_data\n",
    "\n",
    "        self.data_dict = data_dict\n",
    "        dataset = self.produce_dataset(data_dict)\n",
    "\n",
    "        # return 'everything looks fine'\n",
    "        return dataset\n",
    "\n",
    "    def set_label(self, index_data, label_func, label_back_days):\n",
    "        # back n days\n",
    "        # logger.info('label_func = %s'%label_func)\n",
    "        logger.info('label_back_days = %s' % label_back_days)\n",
    "\n",
    "        tmp_column = (index_data['close'].shift(-label_back_days) - index_data['close']) / index_data['close']\n",
    "        # tmp_column = index_data['return_mkt'] # 未来数据\n",
    "        index_data['label'] = tmp_column.map(lambda x: label_func(x))\n",
    "        self.output_size = index_data['label'].value_counts().shape[0]\n",
    "        return index_data\n",
    "\n",
    "    def produce_dataset(self, data_dict):\n",
    "        _ = True\n",
    "        for code, index_data in data_dict.items():\n",
    "            logger.info(code)\n",
    "            basic = self.get_basic_data(code,index_data)\n",
    "            technical = self.get_technical_data(index_data)\n",
    "            output = self.get_output_data(index_data)\n",
    "\n",
    "            self.features[code] = [basic, technical, output]\n",
    "\n",
    "            if _:\n",
    "                all_basic = basic\n",
    "                all_technical = technical\n",
    "                all_output = output\n",
    "                _ = False\n",
    "            else:\n",
    "                all_basic = np.vstack((all_basic, basic))\n",
    "                all_technical = np.vstack((all_technical, technical))\n",
    "                all_output = np.hstack((all_output, output))\n",
    "\n",
    "        # train_dataset\n",
    "        self.all_basic = all_basic\n",
    "        self.all_technical = all_technical\n",
    "        self.all_output = all_output\n",
    "        basic, technical, output = self.cpu_2_device(all_basic, all_technical, all_output)\n",
    "        logger.info(basic.shape)\n",
    "        logger.info(technical.shape)\n",
    "        logger.info(output.shape)\n",
    "        self.dataset = [basic, technical, output]\n",
    "\n",
    "        return self.dataset\n",
    "\n",
    "    def get_input_by_index(self, index_code):\n",
    "        basic, technical, output = self.features[index_code]\n",
    "        basic, technical, output = self.cpu_2_device(basic, technical, output)\n",
    "        return [basic, technical, output]\n",
    "\n",
    "    def cpu_2_device(self, all_basic, all_technical, all_output):\n",
    "        basic = torch.from_numpy(all_basic).float()\n",
    "        technical = torch.from_numpy(all_technical).float()\n",
    "        output = torch.from_numpy(all_output)\n",
    "\n",
    "        basic = basic.to(self.device)\n",
    "        technical = technical.to(self.device)\n",
    "        output = output.to(self.device)\n",
    "        return basic, technical, output\n",
    "\n",
    "    def _normalization(self, index_data):\n",
    "        # normalization\n",
    "        # for column in ['open', 'close', 'high', 'low', 'volume', 'money']:\n",
    "        #     index_data[column], df_max, df_min = DataToolkit.min_max_scale(index_data[column])\n",
    "        for column in []:\n",
    "            index_data[column], df_max, df_min = DataToolkit.standard_scale(index_data[column])\n",
    "        logger.info(index_data.shape)\n",
    "        return index_data\n",
    "\n",
    "    def get_macd_factors(self, index_data: pd.DataFrame):\n",
    "        '''\n",
    "        MACD\n",
    "        https://en.wikipedia.org/wiki/MACD\n",
    "        '''\n",
    "\n",
    "        stock_data = index_data['close']\n",
    "\n",
    "        stock_data_12 = stock_data.rolling(window=12).mean()\n",
    "        stock_data_26 = stock_data.rolling(window=26).mean()\n",
    "\n",
    "        macd_dif = (stock_data_12 - stock_data_26)\n",
    "        macd_macd = (stock_data_12 - stock_data_26).rolling(window=9).mean()\n",
    "\n",
    "        return macd_dif, macd_macd\n",
    "\n",
    "    def get_rsi_factors(self, index_data: pd.DataFrame):\n",
    "        # TODO rsi\n",
    "        close = index_data['close']\n",
    "        real = RSI(close, timeperiod=14)\n",
    "\n",
    "        return real\n",
    "\n",
    "    def get_KDJ_factors(self, index_data: pd.DataFrame):\n",
    "        '''\n",
    "        随机指标\n",
    "        back_n MA Data cal\n",
    "        https://wiki.mbalib.com/wiki/%E9%9A%8F%E6%9C%BA%E6%8C%87%E6%A0%87\n",
    "        '''\n",
    "\n",
    "        stock_data = index_data['close']\n",
    "        close = index_data['close']\n",
    "        high = index_data['high'].rolling(window=9).max()\n",
    "        low = index_data['low'].rolling(window=9).min()\n",
    "\n",
    "        kd_k = (close - low) / (high - low)\n",
    "        kd_d = kd_k.rolling(window=3).mean()\n",
    "        return kd_k, kd_d\n",
    "\n",
    "    def get_DMI_factors(self, index_data: pd.DataFrame):\n",
    "\n",
    "        close = index_data['close']\n",
    "        high = index_data['high']\n",
    "        low = index_data['low']\n",
    "\n",
    "        high_diff = np.maximum(high - high.shift(1), 0)\n",
    "        low_diff = np.maximum(low.shift(1) - low, 0)\n",
    "\n",
    "        dm_positive = high_diff.copy()\n",
    "        dm_negative = low_diff.copy()\n",
    "\n",
    "        dm_positive[high_diff <= low_diff] = 0\n",
    "        dm_negative[high_diff > low_diff] = 0\n",
    "\n",
    "        tr = np.maximum(np.maximum(abs(high - low), abs(low - close.shift(1))),\n",
    "                        abs(high - close.shift(1)))  # .reset_index(drop=True)\n",
    "        # print(tr)\n",
    "        di_positive = dm_positive.rolling(window=14).mean() / tr.rolling(window=14).mean() * 100\n",
    "        di_negative = dm_negative.rolling(window=14).mean() / tr.rolling(window=14).mean() * 100\n",
    "        # print(di_positive, di_negative)\n",
    "        # print(di_positive[di_positive==0].count())\n",
    "        dx = abs(di_positive - di_negative) / abs(di_positive + di_negative)\n",
    "\n",
    "        index_data['dmi_tr_ma14'] = tr\n",
    "        index_data['dmi_di_ma14_positive'] = di_positive\n",
    "        index_data['dmi_di_ma14_negative'] = di_negative\n",
    "        index_data['dmi_dx14'] = dx\n",
    "        index_data['dmi_adx_ma14'] = dx.rolling(window=14).mean()\n",
    "\n",
    "        return index_data\n",
    "\n",
    "    def get_standardized_moment(self, index_data: pd.DataFrame):\n",
    "        '''\n",
    "        偏度与峰度的计算\n",
    "        https://en.wikipedia.org/wiki/Standardized_moment\n",
    "        '''\n",
    "        standardized_moment_data = index_data['return_mkt']\n",
    "        index_data['standardized_moment_skewness'] = standardized_moment_data.rolling(window=30).skew()\n",
    "        index_data['standardized_moment_kurtosis'] = standardized_moment_data.rolling(window=30).kurt()\n",
    "        return index_data\n",
    "\n",
    "    def get_chaikin_oscillator(self, index_data: pd.DataFrame):\n",
    "        '''\n",
    "        蔡金\n",
    "        https://en.wikipedia.org/wiki/Chaikin_Analytics\n",
    "        chaikin_oscillator\n",
    "        '''\n",
    "        Money_Flow_Volume = ((index_data['close'] - index_data['low']) - (index_data['high'] - index_data['close'])) / (\n",
    "                index_data['high'] - index_data['low']) * index_data['volume']\n",
    "        adl = Money_Flow_Volume.cumsum()\n",
    "        index_data['chaikin_oscillator'] = adl.rolling(window=3).mean() - adl.rolling(window=10).mean()\n",
    "        return index_data\n",
    "\n",
    "    #     def get__factors(self,index_data :pd.DataFrame):\n",
    "    #         pass\n",
    "    #     def get__factors(self,index_data :pd.DataFrame):\n",
    "    #         pass\n",
    "    #     def get__factors(self,index_data :pd.DataFrame):\n",
    "    #         pass\n",
    "\n",
    "    def _calculate_features(self, index_data: pd.DataFrame):\n",
    "\n",
    "        # 动力指标 momentum_n\n",
    "        for i in [2, 4, 8, 16]:\n",
    "            for code in self.index_code_list:\n",
    "                index_data[f'momentum_{i}'] = index_data.close - index_data.close.shift(i)\n",
    "            # index_data['momentum']\n",
    "\n",
    "        # 心理线 psychological_line\n",
    "        back_days = 10\n",
    "        index_data['psychological_line'] = 0\n",
    "        for d in range(back_days):\n",
    "            index_data['psychological_line'] += index_data.return_mkt.shift(d).map(lambda x: 1 if x > 0 else 0)\n",
    "        index_data['psychological_line'] = index_data['psychological_line'] / back_days  # *100\n",
    "        # index_data['psychological_line']\n",
    "\n",
    "        # 容量比率 Volumn Ratio\n",
    "        back_days = 5\n",
    "        VUP = index_data.open.map(lambda x: 0)\n",
    "        V = index_data.open.map(lambda x: 0)\n",
    "        for d in range(back_days):\n",
    "            VUP += index_data.return_mkt.shift(d).map(lambda x: 1 if x > 0 else 0) * index_data.volume.shift(d)\n",
    "        for d in range(back_days):\n",
    "            V += index_data.volume.shift(d)\n",
    "        index_data['VR'] = VUP / V  # *100\n",
    "        # index_data['VR']\n",
    "\n",
    "        # calculate kinds of factors\n",
    "        index_data['macd_dif'], index_data['macd_macd'] = self.get_macd_factors(index_data)\n",
    "        index_data['kd_k'], index_data['kd_d'] = self.get_KDJ_factors(index_data)\n",
    "        index_data = self.get_DMI_factors(index_data)\n",
    "        index_data = self.get_standardized_moment(index_data)\n",
    "        index_data = self.get_chaikin_oscillator(index_data)\n",
    "\n",
    "        index_data = DataToolkit.fillna_data(index_data, fillna_drop_narows=True)\n",
    "        return index_data\n",
    "\n",
    "    def fetch_data(self, index_code):\n",
    "        # get database data\n",
    "        m = MysqlAPI.MysqlAPI()\n",
    "        table_column_dict = {'index_data':\n",
    "                                 ['open',\n",
    "                                  'close',\n",
    "                                  'high',\n",
    "                                  'low',\n",
    "                                  'volume',\n",
    "                                  'money',\n",
    "                                  'return_mkt']}\n",
    "\n",
    "        # print(code)\n",
    "        index_data = m.query_index_data(index_code_list=[index_code],\n",
    "                                        trade_date_list=[[self.fromtime, self.endtime]],\n",
    "                                        table_column_dict=table_column_dict)\n",
    "        # remove none\n",
    "        index_data = DataToolkit.fillna_data(index_data, fillna_drop_narows=True)\n",
    "        # print(index_data.shape, data_dict[index_data].shape)\n",
    "\n",
    "        return index_data\n",
    "\n",
    "    def get_tushare_data(self, index_code, index_data):\n",
    "        # get tushare data\n",
    "        dailybasic1 = pro.index_dailybasic(end_date='20070603', ts_code=index_code)\n",
    "        dailybasic2 = pro.index_dailybasic(end_date='20200101', ts_code=index_code)\n",
    "\n",
    "        dailybasic = pd.concat([dailybasic1, dailybasic2])  # .drop_duplicates()\n",
    "\n",
    "        dailybasic.rename(columns={'ts_code': 'index_code'}, inplace=True)\n",
    "\n",
    "        def str_2_time(s):\n",
    "            t = pd.Timestamp(s)\n",
    "            return t\n",
    "\n",
    "        dailybasic['trade_date'] = dailybasic['trade_date'].map(str_2_time)\n",
    "        dailybasic.set_index(['index_code', 'trade_date'], inplace=True)\n",
    "        # merge\n",
    "        index_data = pd.merge(index_data, dailybasic, left_index=True, right_index=True, how='left')\n",
    "        return index_data\n",
    "\n",
    "    def get_technical_data(self, index_data, drop_list=None):\n",
    "        logger.info('GETTING TECHNICAL DATA')\n",
    "        # database data\n",
    "        drop_list = ['label', 'open', 'close', 'high', 'low', 'money']\n",
    "        # tushare data\n",
    "        drop_list += ['pe', 'pb', 'total_mv', 'float_mv', 'total_share', 'float_share', 'free_share']\n",
    "        index_data = index_data.drop(drop_list, axis=1)\n",
    "        self.dynamic_input_size = index_data.shape[1]\n",
    "        logger.info('dynamic_input_size = %s' % self.dynamic_input_size)\n",
    "        # print(index_data.head())\n",
    "        _ = DataToolkit.shift_by_stock_code(index_data, self.days_for_train - 1, 'forward')\n",
    "        print(_.shape)\n",
    "        all_technical = _.values.reshape(_.shape[0], self.dynamic_input_size, self.days_for_train)[:-1]\n",
    "        all_technical = all_technical.swapaxes(1, 2)\n",
    "        print(all_technical.shape)\n",
    "        return all_technical\n",
    "\n",
    "    def get_basic_data(self, index_code, index_data):\n",
    "        logger.info('GETTING BASIC DATA')\n",
    "        basic_list = ['pb', 'pe', 'pe_ttm', 'turnover_rate', 'turnover_rate_f']\n",
    "        normalization_list = ['total_mv', 'float_mv', 'total_share', 'float_share', 'free_share','close']\n",
    "        \n",
    "\n",
    "        if index_code not in self.normalization_parameters:\n",
    "            self.normalization_parameters[index_code] = {}\n",
    "        \n",
    "        for column in normalization_list:\n",
    "            if column not in self.normalization_parameters[index_code]:\n",
    "                index_data[column], df_max, df_min = DataToolkit.min_max_scale(index_data[column])\n",
    "                self.normalization_parameters[index_code][column] = [df_max, df_min]\n",
    "                print(column)\n",
    "                print(df_max, df_min)\n",
    "            else:\n",
    "                df_max, df_min = self.normalization_parameters[index_code][column]\n",
    "                index_data[column], df_max, df_min = DataToolkit.min_max_scale(index_data[column], df_max=df_max, df_min=df_min)\n",
    "        \n",
    "        all_basic = index_data[basic_list + normalization_list]\n",
    "        all_basic = all_basic[self.days_for_train:].values\n",
    "        self.basic_input_size = all_basic.shape[1]\n",
    "        return all_basic\n",
    "\n",
    "    def get_output_data(self, index_data):\n",
    "        logger.info('GETTING OUTPUT DATA')\n",
    "        # construct label features\n",
    "        output = index_data['label'][self.days_for_train:]\n",
    "        output = output.values\n",
    "        logger.info(output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:399001.SZ\n",
      "INFO:__main__:label_back_days = 5\n",
      "INFO:__main__:(3487, 26)\n",
      "INFO:__main__:000905.SH\n",
      "INFO:__main__:label_back_days = 5\n",
      "INFO:__main__:(2997, 26)\n",
      "INFO:__main__:000001.SH\n",
      "INFO:__main__:label_back_days = 5\n",
      "INFO:__main__:(3487, 26)\n",
      "INFO:__main__:000300.SH\n",
      "INFO:__main__:label_back_days = 5\n",
      "INFO:__main__:(3426, 26)\n",
      "INFO:__main__:399001.SZ\n",
      "INFO:__main__:GETTING BASIC DATA\n",
      "INFO:__main__:GETTING TECHNICAL DATA\n",
      "INFO:__main__:dynamic_input_size = 23\n",
      "INFO:__main__:GETTING OUTPUT DATA\n",
      "INFO:__main__:(3472,)\n",
      "INFO:__main__:000905.SH\n",
      "INFO:__main__:GETTING BASIC DATA\n",
      "INFO:__main__:GETTING TECHNICAL DATA\n",
      "INFO:__main__:dynamic_input_size = 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3435, 345)\n",
      "(3434, 15, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:GETTING OUTPUT DATA\n",
      "INFO:__main__:(2982,)\n",
      "INFO:__main__:000001.SH\n",
      "INFO:__main__:GETTING BASIC DATA\n",
      "INFO:__main__:GETTING TECHNICAL DATA\n",
      "INFO:__main__:dynamic_input_size = 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2945, 345)\n",
      "(2944, 15, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:GETTING OUTPUT DATA\n",
      "INFO:__main__:(3472,)\n",
      "INFO:__main__:000300.SH\n",
      "INFO:__main__:GETTING BASIC DATA\n",
      "INFO:__main__:GETTING TECHNICAL DATA\n",
      "INFO:__main__:dynamic_input_size = 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3435, 345)\n",
      "(3434, 15, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:GETTING OUTPUT DATA\n",
      "INFO:__main__:(3411,)\n",
      "INFO:__main__:torch.Size([13337, 10])\n",
      "INFO:__main__:torch.Size([13185, 15, 23])\n",
      "INFO:__main__:torch.Size([13337])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3374, 345)\n",
      "(3373, 15, 23)\n",
      "[[ 2.16       14.36       12.87       ... -0.03826821 -0.05046827\n",
      "  -0.05714737]\n",
      " [ 2.15       14.33       12.85       ... -0.03826821 -0.05046827\n",
      "  -0.05714737]\n",
      " [ 2.16       14.36       12.88       ... -0.03826821 -0.05046827\n",
      "  -0.05714737]\n",
      " ...\n",
      " [ 1.49       12.84       12.48       ...  0.99822568  0.9813687\n",
      "   0.97791415]\n",
      " [ 1.48       12.8        12.44       ...  0.9999973   0.98321114\n",
      "   0.97958727]\n",
      " [ 1.52       13.11       12.74       ...  1.          0.98371623\n",
      "   0.98066164]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data_class = GetData_index(fromtime='2005-01-01',\n",
    "                               index_code_list = ['399001.SZ','000905.SH','000001.SH','000300.SH']\n",
    "                              )\n",
    "    data_class.generate()\n",
    "\n",
    "    print(data_class.all_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_class.t_all_basic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "env_wy",
   "language": "python",
   "name": "env_wy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
