{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm,tqdm_notebook, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyhanlp import *\n",
    "\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig \n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "import platform \n",
    "print(platform.python_version())\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "setup_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the task to train.\n",
    "TASK_NAME = '2c_wash'\n",
    "fname = '2c_dataset.tsv'\n",
    "fromtime = '2016-01-01'\n",
    "endtime = '2019-01-10'\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization.\n",
    "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
    "#datapath = f'/home/wy506wd/data/{TASK_NAME}/'\n",
    "datapath = os.path.join('/home/wy506wd/data/',TASK_NAME)\n",
    "data_dir = datapath\n",
    "\n",
    "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
    "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
    "# bert-base-multilingual-cased, bert-base-chinese.\n",
    "BERT_MODEL = 'chinese_wwm_ext_pytorch'\n",
    "#model_dir = f'/home/wy506wd/download/{BERT_MODEL}/'\n",
    "model_dir = os.path.join('/home/wy506wd/download/',BERT_MODEL)\n",
    "\n",
    "\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "output_model_file = os.path.join(data_dir, WEIGHTS_NAME)\n",
    "# if not os.path.exists(output_model_file):\n",
    "#     os.mkdir(output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-344c28e5ffc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m raw = m.query_jy_news_data(date_list=[[fromtime,endtime]],\n\u001b[0;32m---> 11\u001b[0;31m                             table_column_dict=table_column_dict)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/news_alert/DataQuery/MysqlAPI.py\u001b[0m in \u001b[0;36mquery_jy_news_data\u001b[0;34m(self, date_list, table_column_dict)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mdatabase_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectionPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjy_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/news_alert/DataQuery/ConnectionPool.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconn_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/news_alert/DataQuery/ConnectionPool.py\u001b[0m in \u001b[0;36m_create_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m                            \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                            \u001b[0mpasswd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                            port=self.kwargs.get('port'))\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_put_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/__init__.py\u001b[0m in \u001b[0;36mConnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconnections\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_orig_conn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, user, password, database, port, unix_socket, charset, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, ssl, read_default_group, compress, named_pipe, autocommit, db, passwd, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_ssl_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msslp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, sock)\u001b[0m\n\u001b[1;32m    596\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_seq_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_authentication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_get_server_information\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_server_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0mpacket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mbuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0mpacket_header\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0;31m#if DEBUG: dump_packet(packet_header)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(self, num_bytes)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_wy/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append('./dataquery')\n",
    "import DataQuery.MysqlAPI as MysqlAPI\n",
    "import DataQuery.DataToolkit as DataToolkit \n",
    "\n",
    "#load database data\n",
    "m = MysqlAPI.MysqlAPI()\n",
    "table_column_dict = {'lc_news': \n",
    "                     ['InfoPublDate',\n",
    "                      'InfoTitle', \n",
    "                      'Content',\n",
    "                      'RecordDate',\n",
    "                      'XGRQ']}\n",
    "\n",
    "raw = m.query_jy_news_data(date_list=[[fromtime,endtime]],\n",
    "                            table_column_dict=table_column_dict)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract InvolvedStock (code & name)\n",
    "print(raw.shape)\n",
    "\n",
    "def get_stockcode_from_text(text):\n",
    "    i = re.search('\\(\\d+',text)\n",
    "    if not i: return None\n",
    "    num = i.group(0)[1:]\n",
    "    return num\n",
    "\n",
    "def get_InvolvedStock(df):\n",
    "    df['code_num'] = df['Content'].map(get_stockcode_from_text)\n",
    "    # drop None data\n",
    "    df = DataToolkit.fillna_data(df,fillna_drop_narows=['code_num'])\n",
    "    info_df = m.query_one_table(database='news',\n",
    "                  table_name='stock_info', \n",
    "                  columns=['stock_code','display_name'])\n",
    "    info_df['code_num'] = info_df['stock_code'].map(lambda x: x[:-3])\n",
    "    df = pd.merge(df,info_df,how='inner')\n",
    "    return df\n",
    "\n",
    "data = get_InvolvedStock(raw)\n",
    "# print(get_stockcode_from_text(text))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = DataToolkit.fillna_data(data,fillna_drop_narows=['stock_code'])\n",
    "\n",
    "# align_date    \n",
    "def align_date(mydatetime):\n",
    "    if(mydatetime.time() <= datetime.time(9,30)):\n",
    "        return mydatetime.date()\n",
    "    if(mydatetime.time() >= datetime.time(15,0)):\n",
    "        return mydatetime.date() + datetime.timedelta(days=1)\n",
    "    return None\n",
    "\n",
    "def test_date_distribution(mydatetime):\n",
    "    if(mydatetime.time() <= datetime.time(9,30)):\n",
    "        return 1\n",
    "    if(mydatetime.time() >= datetime.time(15,0)):\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "_ = d.shape[0]\n",
    "print(d.shape)\n",
    "d['RecordDate'] = d['RecordDate'].map(align_date)\n",
    "d = DataToolkit.fillna_data(d,fillna_drop_narows=['RecordDate'])\n",
    "print('num_news_during_marketsOpen = ',_ - d.shape[0])\n",
    "d['RecordDate'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d['RecordDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index\n",
    "d.rename(columns={'RecordDate':'trade_date','InfoTitle':'title','Content':'content'},inplace=True)\n",
    "d['trade_date'] = d['trade_date'].map(lambda x : x.strftime(\"%Y-%m-%d\"))\n",
    "d.set_index(['stock_code','trade_date'],inplace=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capm.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方向标记\n",
    "# merge data together\n",
    "dataset = pd.DataFrame([])\n",
    "\n",
    "for index in ['000001.SH','399107.SZ']:\n",
    "    table_column_dict = {'capm': \n",
    "                         ['return_adj_d001',\n",
    "                          'car_hs300_b30_d001']}\n",
    "\n",
    "    capm = m.query_trading_data(index_code=index,\n",
    "                                trade_date_list=[[fromtime,endtime]],\n",
    "                                table_column_dict=table_column_dict)\n",
    "    \n",
    "    tmp = pd.merge(d,capm,left_index=True,right_index=True,how='inner')\n",
    "    dataset = pd.concat([dataset,tmp]).drop_duplicates()\n",
    "    print(tmp.shape)\n",
    "    \n",
    "print(dataset.shape)\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label news according to markets data\n",
    "def label_direction_3(trend):\n",
    "    if abs(trend) <= 0.02:\n",
    "        return 1\n",
    "    return 2 if trend > 0 else 0\n",
    "\n",
    "def label_direction_wy(trend):\n",
    "    for index, i in enumerate([-0.005,0.005]):\n",
    "        if trend < i:\n",
    "            return index\n",
    "    return index+1\n",
    "\n",
    "def label_direction_5(trend):\n",
    "    for index, i in enumerate([-0.02,-0.005,0.005,0.02]):\n",
    "        if trend < i:\n",
    "            return index\n",
    "    #print(trend)\n",
    "    return index+1\n",
    "    \n",
    "\n",
    "\n",
    "def label_direction_2(trend):\n",
    "    return 0 if trend <= 0 else 1\n",
    "\n",
    "\n",
    "\n",
    "dataset['label'] = dataset['return_adj_d001'].map(label_direction_2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# raw data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dataset[['ID','label','content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Kth_paragraph(content,k=0):\n",
    "    '''\n",
    "    return Kth paragraph\n",
    "    '''\n",
    "    return content.split('\\n')[k]\n",
    "\n",
    "def clean_bracket(text):\n",
    "    text =  text.replace('\\n','')\n",
    "    if len(text) <= MAX_SEQ_LENGTH:\n",
    "        return text\n",
    "    return re.sub('[\\(\\（]\\S*?[\\)\\）]','',text)\n",
    "\n",
    "def _getSummary(text):\n",
    "    return '。'.join(HanLP.extractSummary(clean_bracket(text), 5))\n",
    "\n",
    "tmp['content'] = tmp['content'].map(_getSummary)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncheck_key_words('我买的股票都跌停了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncheck_key_words(txt):\n",
    "    key_words = ['跌停','跌幅','涨幅','涨停']\n",
    "    #print(type(txt))\n",
    "    for k in key_words:\n",
    "        if txt.find(k) != -1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _k(x):\n",
    "    return uncheck_key_words(x['content'])\n",
    "\n",
    "logger.info(tmp.shape)\n",
    "tmp = tmp[tmp['content'].apply(uncheck_key_words)]\n",
    "logger.info(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance train data\n",
    "# 下采样平衡数据\n",
    "# down sample\n",
    "def lower_sample_data_by_sample(df,percent=1):\n",
    "    most_data = df[df['label'] == 0]  # 多数类别的样本\n",
    "    minority_data = df[df['label'] == 1]  # 少数类别的样本   \n",
    "    #random sample most_data\n",
    "    lower_data=most_data.sample(n=int(percent*len(minority_data)))   \n",
    "    return (pd.concat([lower_data,minority_data]))\n",
    "# tmp = lower_sample_data_by_sample(tmp)\n",
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tmp['label'].value_counts()\n",
    "logger.info('\\n'+ str(a))\n",
    "acc_1class = a.max()/a.sum()\n",
    "logger.info(acc_1class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save raw training data\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "tmp.to_csv(os.path.join(data_dir,fname),sep='\\t',index = False)\n",
    "tmp.to_csv(os.path.join(data_dir,\"train.tsv\"),sep='\\t',index = False,header=None)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert  .csv  to  examples  to  features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The output directory where the fine-tuned model and checkpoints will be written.\n",
    "# OUTPUT_DIR = f'{datapath}/{TASK_NAME}/'\n",
    "OUTPUT_DIR = os.path.join(datapath,TASK_NAME)\n",
    "\n",
    "# The directory where the evaluation reports will be written to.\n",
    "# REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
    "\n",
    "# This is where BERT will look for pre-trained models to load parameters from.\n",
    "# CACHE_DIR = 'cache/'\n",
    "\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "RANDOM_SEED = 42\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WARMUP_PROPORTION = 0.1\n",
    "OUTPUT_MODE = 'classification'\n",
    "CONFIG_NAME = \"config.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "class MultiClassificationProcessor(DataProcessor):\n",
    "    \"\"\"Processor for binary classification dataset.\"\"\"\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[2]\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if OUTPUT_MODE == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        elif OUTPUT_MODE == \"regression\":\n",
    "            label_id = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(OUTPUT_MODE)\n",
    "            \n",
    "        if ex_index < 5: # show examples like dataframe.head(5)\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>505245187020</td>\n",
       "      <td>1</td>\n",
       "      <td>浦发银行5日傍晚发布其2015年度业绩快报。2015年浦发银行实现营业收入1。上一年度每股收...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505245187037</td>\n",
       "      <td>1</td>\n",
       "      <td>同比增长18.97%。公司实现营业收入1465.43亿元。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505245187012</td>\n",
       "      <td>1</td>\n",
       "      <td>较上年末同比增长21.14%。　　中国证券网讯 浦发银行1月4日晚间领衔披露沪市两市2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>505251238389</td>\n",
       "      <td>1</td>\n",
       "      <td>较上年末同比增长21.14%。　　【财经网讯】浦发银行1月4日晚间领衔披露沪深两市2015年...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505257374309</td>\n",
       "      <td>1</td>\n",
       "      <td>浦发银行实现归属于母公司股东的净利润505.98亿元。资产规模较2014年末增长20.19%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>505274641771</td>\n",
       "      <td>1</td>\n",
       "      <td>2015年浦发银行的资产规模和营业收入均达到两位数增长。资产规模较2014年末增长20.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>505370423700</td>\n",
       "      <td>0</td>\n",
       "      <td>国金证券银行业分析师马鲲鹏则表示。同比增长7.6%。这也是该行为2016年供给侧改革将驱...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>505533632486</td>\n",
       "      <td>0</td>\n",
       "      <td>资产规模较2014年末增长20.19%。浦发银行在支持实体经济增长的过程中。金融市场业务实现...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>505533632480</td>\n",
       "      <td>0</td>\n",
       "      <td>为用户的移动支付带来全新的便捷体验与更高的金融安全。浦发银行长期以来专注于移动支付创新的服务...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>506138464264</td>\n",
       "      <td>1</td>\n",
       "      <td>浦发银行第九次全行志愿者日活动。　　本次志愿者活动以。　　作为业内率先开展全行志愿者日活动并...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>506484039568</td>\n",
       "      <td>0</td>\n",
       "      <td>本次志愿者活动以。浦发银行自2008年1月开始。每年1月9日行庆前后都会在全国范围内开展志愿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>506644866017</td>\n",
       "      <td>1</td>\n",
       "      <td>公司于近日相继获得中国银监会和中国人民银行同意公司发行绿色金融债券的批文。公司绿色金融债券发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>507175227905</td>\n",
       "      <td>0</td>\n",
       "      <td>的理念被每一位浦发银行北京海淀园支行员工积极践行。海淀园支行在开拓对公业务上也运用了。海淀园...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>507232307035</td>\n",
       "      <td>1</td>\n",
       "      <td>确立了金融机构发行绿色金融债券并专项支持绿色产业项目的制度框架。浦发银行将基于发行前筛选确定...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>507235216316</td>\n",
       "      <td>1</td>\n",
       "      <td>公司与中国移动于1月26日签订了新五年战略合作协议。深化双方战略合作关系。提升战略合作对双方...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>507246020272</td>\n",
       "      <td>1</td>\n",
       "      <td>绿色金融债券是金融机构法人依法在银行间债券市场发行的、募集资金用于支持绿色产业项目。央行在银...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>507249606172</td>\n",
       "      <td>1</td>\n",
       "      <td>绿色金融债券是金融机构法人依法在银行间债券市场发行的、募集资金用于支持绿色产业项目。此次浦发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>508816807475</td>\n",
       "      <td>0</td>\n",
       "      <td>浦发银行此次发行的绿色金融债券。浦发银行成功发行境内首单绿色金融债券。央行刚确立了金融机构发...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>508880452750</td>\n",
       "      <td>0</td>\n",
       "      <td>浦发银行2月15日晚间公告称。因公司正在筹划涉及非公开发行普通股股票的重大事项。并于停牌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>508887605048</td>\n",
       "      <td>0</td>\n",
       "      <td>公司普通股及优先股股票自2016年2月16日起停牌。公司正在筹划涉及非公开发行普通股股票的重...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>510085219892</td>\n",
       "      <td>0</td>\n",
       "      <td>公司正在筹划涉及非公开发行普通股股票的重大事项。公司还尚未就发行对象、发行金额、发行价格等事...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>510333614892</td>\n",
       "      <td>0</td>\n",
       "      <td>浦发银行上海分行也为YH打造了一套投行业务解决方案。为公司提供了一整套的资金管理解决方案。使...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>510706211492</td>\n",
       "      <td>0</td>\n",
       "      <td>公司因筹划涉及非公开发行普通股股票的重大事项自2016年2月16日起停牌。因公司正在筹划涉及...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>510948034992</td>\n",
       "      <td>0</td>\n",
       "      <td>浦发大厦坍塌事故或因楼顶绿化施工所致。位于济南市黑虎泉西路 139 号的上海浦东发展银行股份...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>510955819722</td>\n",
       "      <td>0</td>\n",
       "      <td>向公司合并持股第一大股东国际集团及其子公司国鑫投资非公开发行不超过92169.049万股。国...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>510956409854</td>\n",
       "      <td>0</td>\n",
       "      <td>本次发行的募集资金扣除发行费用后将全部用于补充公司的核心一级资本。经营战略实施的资本需求...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>511221007933</td>\n",
       "      <td>1</td>\n",
       "      <td>虽然从持股比例上看生命人寿已经成为浦发银行的重要股东之一。这一占股比例与大股东上海国际集...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>511495230845</td>\n",
       "      <td>1</td>\n",
       "      <td>完成了上海信托97.33%股权过户事宜。购买其合计持有的上海信托97.33%股权。上海信托成...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>511828216068</td>\n",
       "      <td>1</td>\n",
       "      <td>此次浦发银行要整合的并非针对个人的电商平台。浦发银行将为电商平台提供信用增级、资金监管、交易...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>511830014356</td>\n",
       "      <td>1</td>\n",
       "      <td>该方案将为电商平台客户提供信用增级、资金监管、交易场景、支付体系、账户管理、会员服务、融资贸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44061</th>\n",
       "      <td>582132311194</td>\n",
       "      <td>0</td>\n",
       "      <td>但公司已有乘用车动力总成技术储备。　　有投资者提问公司为哪些轿车品牌提供动力总成。公司将积极...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44062</th>\n",
       "      <td>586219513195</td>\n",
       "      <td>0</td>\n",
       "      <td>预计年底前可实现小批量的试制</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44063</th>\n",
       "      <td>587162403789</td>\n",
       "      <td>1</td>\n",
       "      <td>致力于和更多客户开展合作。　　越博动力(300742)8月9日在互动平台回复投资者提问时称</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44064</th>\n",
       "      <td>592503317353</td>\n",
       "      <td>1</td>\n",
       "      <td>正积极推进更进一步的合作</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44065</th>\n",
       "      <td>597256809134</td>\n",
       "      <td>0</td>\n",
       "      <td>公司拟在河南省郑州市、四川省成都市、陕西省西安市新设立三家全资子公司。有利于公司进一步向新能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44066</th>\n",
       "      <td>588898827389</td>\n",
       "      <td>0</td>\n",
       "      <td>实现净利润2024万元。公司年产2.2亿平方米高性能热转印成像材料扩建项目和全球营销中心建设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44067</th>\n",
       "      <td>589973112546</td>\n",
       "      <td>1</td>\n",
       "      <td>公司与威马汽车有项目在前期接洽中</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44068</th>\n",
       "      <td>590490312114</td>\n",
       "      <td>1</td>\n",
       "      <td>暂无法披露具体信息</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44069</th>\n",
       "      <td>587574308420</td>\n",
       "      <td>0</td>\n",
       "      <td>公司目前正在积极争取参与杭州亚运会的相关项目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44070</th>\n",
       "      <td>592267814206</td>\n",
       "      <td>0</td>\n",
       "      <td>标的资产为杭州市城乡建设设计院股份有限公司</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44071</th>\n",
       "      <td>600292255213</td>\n",
       "      <td>0</td>\n",
       "      <td>公司与杭州拱墅投资发展有限公司签订人才专项租赁房项目工程EPC承包合同。公司中标杭州铁路北站...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44072</th>\n",
       "      <td>593111424782</td>\n",
       "      <td>1</td>\n",
       "      <td>该项目的结题验收证明锐科激光已具备20kW级高功率光纤激光器研制生产能力。项目在国内首次实现...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44073</th>\n",
       "      <td>599093117141</td>\n",
       "      <td>0</td>\n",
       "      <td>另一方面是充分发挥凯迪仕与本公司在智能锁业务上的协同效应。将发挥凯迪仕与其在智能锁业务上的协...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44074</th>\n",
       "      <td>599177110493</td>\n",
       "      <td>0</td>\n",
       "      <td>拥有自主知识产权的智能工厂和智能物流系统的管控平台。这将为顶固集创在未来的市场发展中提供更广...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44075</th>\n",
       "      <td>584485216856</td>\n",
       "      <td>1</td>\n",
       "      <td>公司计划于欧洲建设海外生产研发基地。公司欧洲生产研发基地建设项目将分期进行。宁德时代同时公告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44076</th>\n",
       "      <td>584615414108</td>\n",
       "      <td>1</td>\n",
       "      <td>长城汽车与宝马控股成立合资公司光束汽车。公司与宝马股份的全资子公司宝马控股公司在德国柏林...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44077</th>\n",
       "      <td>584640019525</td>\n",
       "      <td>0</td>\n",
       "      <td>公司的主营业务收入具有较明显的季节性特征。主要是因为下游新能源汽车生产也具有季节性特征。　　...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44078</th>\n",
       "      <td>584732411813</td>\n",
       "      <td>1</td>\n",
       "      <td>业绩变动主要原因是上年同期转让了普莱德的股权取得的处置收益影响。同比增长31.43%-39....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44079</th>\n",
       "      <td>584742012712</td>\n",
       "      <td>1</td>\n",
       "      <td>溧阳项目正在正常实施中</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44080</th>\n",
       "      <td>586257609169</td>\n",
       "      <td>0</td>\n",
       "      <td>该协议并未达到公司披露的标准。为什么不公告呢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44081</th>\n",
       "      <td>587162403792</td>\n",
       "      <td>0</td>\n",
       "      <td>公司拟与金融机构开展外汇套期保值业务。规模不超过56亿元人民币或等值外币金额。随着公司海外业...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44082</th>\n",
       "      <td>588374117065</td>\n",
       "      <td>1</td>\n",
       "      <td>同比下降49.70%。同比增长48.69%。下降的主要原因是上年同期转让了持有的普莱德新能源...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44083</th>\n",
       "      <td>592941627144</td>\n",
       "      <td>1</td>\n",
       "      <td>一机构席位卖出1053万元。龙虎榜数据显示</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44084</th>\n",
       "      <td>596753713237</td>\n",
       "      <td>1</td>\n",
       "      <td>e公司讯。公司及子公司拟向银行申请总计不超过1100亿元的综合授信额度。宁德时代(300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44085</th>\n",
       "      <td>598659012281</td>\n",
       "      <td>0</td>\n",
       "      <td>公司与浙江吉润汽车有限公司拟共同出资设立合资公司。\"宁德时代新能源科技股份有限公司总裁、CE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44086</th>\n",
       "      <td>598659012611</td>\n",
       "      <td>0</td>\n",
       "      <td>公司出资5.1亿元。　　e公司讯。合资公司注册资本10亿元。公司与浙江吉润拟共同设立合资公司...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44087</th>\n",
       "      <td>598992249525</td>\n",
       "      <td>0</td>\n",
       "      <td>此次供应商大会上。且是隔膜供货商中的唯一获奖者</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44088</th>\n",
       "      <td>594840015155</td>\n",
       "      <td>0</td>\n",
       "      <td>一机构席位卖出1472万元。　　e公司讯。成创业板市值第二大公司。该股盘后数据显示</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44089</th>\n",
       "      <td>597352215939</td>\n",
       "      <td>0</td>\n",
       "      <td>拟在光明区投资建设迈瑞上市公司总部及制造基地项目。公司拟通过本次合作。公司与深圳市光明区政府...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44090</th>\n",
       "      <td>598439712752</td>\n",
       "      <td>0</td>\n",
       "      <td>加之公司大部分产品向经销商销售以后。公司产品受。两票制。　　e公司讯。由经销商直接销售给终端客户</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID  label                                            content\n",
       "0      505245187020      1  浦发银行5日傍晚发布其2015年度业绩快报。2015年浦发银行实现营业收入1。上一年度每股收...\n",
       "1      505245187037      1                     同比增长18.97%。公司实现营业收入1465.43亿元。　\n",
       "2      505245187012      1  较上年末同比增长21.14%。　　中国证券网讯 浦发银行1月4日晚间领衔披露沪市两市2015...\n",
       "3      505251238389      1  较上年末同比增长21.14%。　　【财经网讯】浦发银行1月4日晚间领衔披露沪深两市2015年...\n",
       "4      505257374309      1  浦发银行实现归属于母公司股东的净利润505.98亿元。资产规模较2014年末增长20.19%...\n",
       "5      505274641771      1  2015年浦发银行的资产规模和营业收入均达到两位数增长。资产规模较2014年末增长20.19...\n",
       "6      505370423700      0  　　国金证券银行业分析师马鲲鹏则表示。同比增长7.6%。这也是该行为2016年供给侧改革将驱...\n",
       "7      505533632486      0  资产规模较2014年末增长20.19%。浦发银行在支持实体经济增长的过程中。金融市场业务实现...\n",
       "8      505533632480      0  为用户的移动支付带来全新的便捷体验与更高的金融安全。浦发银行长期以来专注于移动支付创新的服务...\n",
       "9      506138464264      1  浦发银行第九次全行志愿者日活动。　　本次志愿者活动以。　　作为业内率先开展全行志愿者日活动并...\n",
       "10     506484039568      0  本次志愿者活动以。浦发银行自2008年1月开始。每年1月9日行庆前后都会在全国范围内开展志愿...\n",
       "11     506644866017      1  公司于近日相继获得中国银监会和中国人民银行同意公司发行绿色金融债券的批文。公司绿色金融债券发...\n",
       "12     507175227905      0  的理念被每一位浦发银行北京海淀园支行员工积极践行。海淀园支行在开拓对公业务上也运用了。海淀园...\n",
       "13     507232307035      1  确立了金融机构发行绿色金融债券并专项支持绿色产业项目的制度框架。浦发银行将基于发行前筛选确定...\n",
       "14     507235216316      1  公司与中国移动于1月26日签订了新五年战略合作协议。深化双方战略合作关系。提升战略合作对双方...\n",
       "15     507246020272      1  绿色金融债券是金融机构法人依法在银行间债券市场发行的、募集资金用于支持绿色产业项目。央行在银...\n",
       "16     507249606172      1  绿色金融债券是金融机构法人依法在银行间债券市场发行的、募集资金用于支持绿色产业项目。此次浦发...\n",
       "17     508816807475      0  浦发银行此次发行的绿色金融债券。浦发银行成功发行境内首单绿色金融债券。央行刚确立了金融机构发...\n",
       "18     508880452750      0  　　浦发银行2月15日晚间公告称。因公司正在筹划涉及非公开发行普通股股票的重大事项。并于停牌...\n",
       "19     508887605048      0  公司普通股及优先股股票自2016年2月16日起停牌。公司正在筹划涉及非公开发行普通股股票的重...\n",
       "20     510085219892      0  公司正在筹划涉及非公开发行普通股股票的重大事项。公司还尚未就发行对象、发行金额、发行价格等事...\n",
       "21     510333614892      0  浦发银行上海分行也为YH打造了一套投行业务解决方案。为公司提供了一整套的资金管理解决方案。使...\n",
       "22     510706211492      0  公司因筹划涉及非公开发行普通股股票的重大事项自2016年2月16日起停牌。因公司正在筹划涉及...\n",
       "23     510948034992      0  浦发大厦坍塌事故或因楼顶绿化施工所致。位于济南市黑虎泉西路 139 号的上海浦东发展银行股份...\n",
       "24     510955819722      0  向公司合并持股第一大股东国际集团及其子公司国鑫投资非公开发行不超过92169.049万股。国...\n",
       "25     510956409854      0  　　本次发行的募集资金扣除发行费用后将全部用于补充公司的核心一级资本。经营战略实施的资本需求...\n",
       "26     511221007933      1  　　虽然从持股比例上看生命人寿已经成为浦发银行的重要股东之一。这一占股比例与大股东上海国际集...\n",
       "27     511495230845      1  完成了上海信托97.33%股权过户事宜。购买其合计持有的上海信托97.33%股权。上海信托成...\n",
       "28     511828216068      1  此次浦发银行要整合的并非针对个人的电商平台。浦发银行将为电商平台提供信用增级、资金监管、交易...\n",
       "29     511830014356      1  该方案将为电商平台客户提供信用增级、资金监管、交易场景、支付体系、账户管理、会员服务、融资贸...\n",
       "...             ...    ...                                                ...\n",
       "44061  582132311194      0  但公司已有乘用车动力总成技术储备。　　有投资者提问公司为哪些轿车品牌提供动力总成。公司将积极...\n",
       "44062  586219513195      0                                     预计年底前可实现小批量的试制\n",
       "44063  587162403789      1      致力于和更多客户开展合作。　　越博动力(300742)8月9日在互动平台回复投资者提问时称\n",
       "44064  592503317353      1                                       正积极推进更进一步的合作\n",
       "44065  597256809134      0  公司拟在河南省郑州市、四川省成都市、陕西省西安市新设立三家全资子公司。有利于公司进一步向新能...\n",
       "44066  588898827389      0  实现净利润2024万元。公司年产2.2亿平方米高性能热转印成像材料扩建项目和全球营销中心建设...\n",
       "44067  589973112546      1                                   公司与威马汽车有项目在前期接洽中\n",
       "44068  590490312114      1                                          暂无法披露具体信息\n",
       "44069  587574308420      0                             公司目前正在积极争取参与杭州亚运会的相关项目\n",
       "44070  592267814206      0                              标的资产为杭州市城乡建设设计院股份有限公司\n",
       "44071  600292255213      0  公司与杭州拱墅投资发展有限公司签订人才专项租赁房项目工程EPC承包合同。公司中标杭州铁路北站...\n",
       "44072  593111424782      1  该项目的结题验收证明锐科激光已具备20kW级高功率光纤激光器研制生产能力。项目在国内首次实现...\n",
       "44073  599093117141      0  另一方面是充分发挥凯迪仕与本公司在智能锁业务上的协同效应。将发挥凯迪仕与其在智能锁业务上的协...\n",
       "44074  599177110493      0  拥有自主知识产权的智能工厂和智能物流系统的管控平台。这将为顶固集创在未来的市场发展中提供更广...\n",
       "44075  584485216856      1  公司计划于欧洲建设海外生产研发基地。公司欧洲生产研发基地建设项目将分期进行。宁德时代同时公告...\n",
       "44076  584615414108      1  　　长城汽车与宝马控股成立合资公司光束汽车。公司与宝马股份的全资子公司宝马控股公司在德国柏林...\n",
       "44077  584640019525      0  公司的主营业务收入具有较明显的季节性特征。主要是因为下游新能源汽车生产也具有季节性特征。　　...\n",
       "44078  584732411813      1  业绩变动主要原因是上年同期转让了普莱德的股权取得的处置收益影响。同比增长31.43%-39....\n",
       "44079  584742012712      1                                        溧阳项目正在正常实施中\n",
       "44080  586257609169      0                             该协议并未达到公司披露的标准。为什么不公告呢\n",
       "44081  587162403792      0  公司拟与金融机构开展外汇套期保值业务。规模不超过56亿元人民币或等值外币金额。随着公司海外业...\n",
       "44082  588374117065      1  同比下降49.70%。同比增长48.69%。下降的主要原因是上年同期转让了持有的普莱德新能源...\n",
       "44083  592941627144      1                              一机构席位卖出1053万元。龙虎榜数据显示\n",
       "44084  596753713237      1  　　e公司讯。公司及子公司拟向银行申请总计不超过1100亿元的综合授信额度。宁德时代(300...\n",
       "44085  598659012281      0  公司与浙江吉润汽车有限公司拟共同出资设立合资公司。\"宁德时代新能源科技股份有限公司总裁、CE...\n",
       "44086  598659012611      0  公司出资5.1亿元。　　e公司讯。合资公司注册资本10亿元。公司与浙江吉润拟共同设立合资公司...\n",
       "44087  598992249525      0                            此次供应商大会上。且是隔膜供货商中的唯一获奖者\n",
       "44088  594840015155      0          一机构席位卖出1472万元。　　e公司讯。成创业板市值第二大公司。该股盘后数据显示\n",
       "44089  597352215939      0  拟在光明区投资建设迈瑞上市公司总部及制造基地项目。公司拟通过本次合作。公司与深圳市光明区政府...\n",
       "44090  598439712752      0   加之公司大部分产品向经销商销售以后。公司产品受。两票制。　　e公司讯。由经销商直接销售给终端客户\n",
       "\n",
       "[44091 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load .tsv data\n",
    "# dat = pd.read_csv(f'{datapath}/{fname}',delimiter=\"\\t\")\n",
    "# dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   Model name '/home/wy506wd/download/chinese_wwm_ext_pytorch' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '/home/wy506wd/download/chinese_wwm_ext_pytorch' is a path or url to a directory containing tokenizer files.\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/wy506wd/download/chinese_wwm_ext_pytorch/added_tokens.json. We won't load it.\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/wy506wd/download/chinese_wwm_ext_pytorch/special_tokens_map.json. We won't load it.\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   Didn't find file /home/wy506wd/download/chinese_wwm_ext_pytorch/tokenizer_config.json. We won't load it.\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   loading file /home/wy506wd/download/chinese_wwm_ext_pytorch/vocab.txt\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/04/2019 21:46:00 - INFO - pytorch_transformers.tokenization_utils -   loading file None\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   *** Example ***\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   guid: train-0\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   tokens: [CLS] 浦 发 银 行 5 日 傍 晚 发 布 其 2015 年 度 业 绩 快 报 。 2015 年 浦 发 银 行 实 现 营 业 收 入 1 。 上 一 年 度 每 股 收 益 是 2 . 52 元 。 同 比 增 长 5 . 56 % 。 基 本 每 股 收 益 为 2 . 66 元 [SEP]\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   input_ids: 101 3855 1355 7213 6121 126 3189 988 3241 1355 2357 1071 8119 2399 2428 689 5327 2571 2845 511 8119 2399 3855 1355 7213 6121 2141 4385 5852 689 3119 1057 122 511 677 671 2399 2428 3680 5500 3119 4660 3221 123 119 8247 1039 511 1398 3683 1872 7270 126 119 8259 110 511 1825 3315 3680 5500 3119 4660 711 123 119 8347 1039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:00 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   *** Example ***\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   guid: train-1\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   tokens: [CLS] 同 比 增 长 18 . 97 % 。 公 司 实 现 营 业 收 入 146 ##5 . 43 亿 元 。 [SEP]\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_ids: 101 1398 3683 1872 7270 8123 119 8380 110 511 1062 1385 2141 4385 5852 689 3119 1057 9630 8157 119 8250 783 1039 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   *** Example ***\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   guid: train-2\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   tokens: [CLS] 较 上 年 末 同 比 增 长 21 . 14 % 。 中 国 证 券 网 讯 浦 发 银 行 1 月 4 日 晚 间 领 衔 披 露 沪 市 两 市 2015 年 度 首 份 业 绩 快 报 。 同 比 增 长 7 . 60 % 。 归 属 于 母 公 司 股 东 的 净 资 产 315 ##1 . 65 亿 元 。 归 属 于 母 公 司 普 通 股 股 东 的 每 股 净 资 产 为 15 . 29 元 [SEP]\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_ids: 101 6772 677 2399 3314 1398 3683 1872 7270 8128 119 8122 110 511 704 1744 6395 1171 5381 6380 3855 1355 7213 6121 122 3299 125 3189 3241 7313 7566 6124 2847 7463 3772 2356 697 2356 8119 2399 2428 7674 819 689 5327 2571 2845 511 1398 3683 1872 7270 128 119 8183 110 511 2495 2247 754 3678 1062 1385 5500 691 4638 1112 6598 772 9613 8148 119 8284 783 1039 511 2495 2247 754 3678 1062 1385 3249 6858 5500 5500 691 4638 3680 5500 1112 6598 772 711 8115 119 8162 1039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   *** Example ***\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   guid: train-3\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   tokens: [CLS] 较 上 年 末 同 比 增 长 21 . 14 % 。 【 财 经 网 讯 】 浦 发 银 行 1 月 4 日 晚 间 领 衔 披 露 沪 深 两 市 2015 年 度 首 份 业 绩 快 报 。 回 看 2014 年 度 业 绩 快 报 。 同 比 增 长 7 . 60 % 。 公 司 2015 年 度 实 现 营 业 收 入 146 ##5 . 43 亿 元 [SEP]\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_ids: 101 6772 677 2399 3314 1398 3683 1872 7270 8128 119 8122 110 511 523 6568 5307 5381 6380 524 3855 1355 7213 6121 122 3299 125 3189 3241 7313 7566 6124 2847 7463 3772 3918 697 2356 8119 2399 2428 7674 819 689 5327 2571 2845 511 1726 4692 8127 2399 2428 689 5327 2571 2845 511 1398 3683 1872 7270 128 119 8183 110 511 1062 1385 8119 2399 2428 2141 4385 5852 689 3119 1057 9630 8157 119 8250 783 1039 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   label: 1 (id = 1)\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   *** Example ***\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   guid: train-4\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   tokens: [CLS] 浦 发 银 行 实 现 归 属 于 母 公 司 股 东 的 净 利 润 505 . 98 亿 元 。 资 产 规 模 较 2014 年 末 增 长 20 . 19 % 。 较 2014 年 增 长 5 . 56 % 。 全 年 实 现 营 业 收 入 1 。 年 末 不 良 贷 款 率 1 . 56 % [SEP]\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   input_ids: 101 3855 1355 7213 6121 2141 4385 2495 2247 754 3678 1062 1385 5500 691 4638 1112 1164 3883 11033 119 8327 783 1039 511 6598 772 6226 3563 6772 8127 2399 3314 1872 7270 8113 119 8131 110 511 6772 8127 2399 1872 7270 126 119 8259 110 511 1059 2399 2141 4385 5852 689 3119 1057 122 511 2399 3314 679 5679 6587 3621 4372 122 119 8259 110 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2019 21:46:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "11/04/2019 21:46:01 - INFO - __main__ -   label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(os.path.join('/home/wy506wd/download/',BERT_MODEL))\n",
    "processor = MultiClassificationProcessor()\n",
    "\n",
    "# convert examples 2 features\n",
    "train_examples = processor.get_train_examples(data_dir)\n",
    "train_examples_len = len(train_examples)\n",
    "label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "num_labels = len(label_list)\n",
    "\n",
    "num_train_optimization_steps = int(\n",
    "    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\n",
    "\n",
    "train_features = convert_examples_to_features(train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained(model_dir)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2019 21:46:33 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /home/wy506wd/download/chinese_wwm_ext_pytorch/config.json\n",
      "11/04/2019 21:46:33 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "11/04/2019 21:46:33 - INFO - pytorch_transformers.modeling_utils -   loading weights file /home/wy506wd/download/chinese_wwm_ext_pytorch/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(os.path.join(model_dir, \"config.json\"))\n",
    "#os.path.join(model_dir, \"train.tsv\")\n",
    "model = BertForSequenceClassification(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#model.freeze_bert_encoder()\n",
    "#model.classifier.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_steps = 1\n",
    "weight_decay = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "nb_tr_steps = 0\n",
    "tr_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2019 21:46:47 - INFO - __main__ -   ***** Running training *****\n",
      "11/04/2019 21:46:47 - INFO - __main__ -     Num examples = 44091\n",
      "11/04/2019 21:46:47 - INFO - __main__ -     Batch size = 16\n",
      "11/04/2019 21:46:47 - INFO - __main__ -     Num steps = 13775\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", train_examples_len)\n",
    "logger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "if OUTPUT_MODE == \"classification\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "elif OUTPUT_MODE == \"regression\":\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_,model):\n",
    "    model.train()\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    \n",
    "    train_sampler = RandomSampler(sub_train_)\n",
    "    data = DataLoader(sub_train_, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "    for i, batch in enumerate(tqdm(data,desc='TRAIN')):\n",
    "        optimizer.zero_grad()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        #print(input_ids)\n",
    "        output = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "        loss = criterion(output, label_ids)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == label_ids).sum().item()\n",
    "        #print(f'\\r{i*TRAIN_BATCH_SIZE/len(sub_train_)}')\n",
    "        \n",
    "        labels += label_ids.cpu().numpy().tolist()\n",
    "        preds += output.argmax(1).cpu().numpy().tolist()\n",
    "        #break\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    # eval\n",
    "    result = get_eval_report(TASK_NAME, labels, preds)\n",
    "    result['loss'] = train_loss / len(sub_train_)\n",
    "    result['acc'] = train_acc / len(sub_train_)\n",
    "    show_eval_report(result,'train')\n",
    "    return result\n",
    "\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "def get_eval_report(task_name, labels, preds):\n",
    "    assert len(preds) == len(labels)\n",
    "    # processor.get_labels()\n",
    "    mcm = multilabel_confusion_matrix(labels, preds, labels=list(range(num_labels)))\n",
    "    return {\n",
    "        \"task\": task_name,\n",
    "        \"mcm\": mcm\n",
    "    }\n",
    "\n",
    "def show_eval_report(result,name = ''):\n",
    "    logger.info(\"***** Eval %s results *****\" %name )\n",
    "    for key in (result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        \n",
    "def test(data_, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    labels = []\n",
    "    preds = []\n",
    "    sampler = SequentialSampler(data_)\n",
    "    data = DataLoader(data_, sampler=sampler, batch_size=TRAIN_BATCH_SIZE)\n",
    "    for batch in data:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "            loss = criterion(output, label_ids)\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (output.argmax(1) == label_ids).sum().item()\n",
    "        labels += label_ids.cpu().numpy().tolist()\n",
    "        #print(output.argmax(1).cpu().numpy().tolist())\n",
    "        preds += output.argmax(1).cpu().numpy().tolist()\n",
    "    \n",
    "    result = get_eval_report(TASK_NAME, labels, preds)\n",
    "    result['loss'] = total_loss / len(data_)\n",
    "    result['acc'] = total_acc / len(data_)\n",
    "    show_eval_report(result,'test')      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.9)\n",
    "test_len = len(train_dataset) - train_len\n",
    "sub_train_, sub_test_ = random_split(train_dataset, [train_len, test_len])\n",
    "\n",
    "min_test_loss = float('inf')\n",
    "best_acc = -float('inf')\n",
    "\n",
    "train_results = []\n",
    "test_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.604069\n",
       "1    0.395931\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_label_ids.cpu().numpy())[0].value_counts(sort=False,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS function\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.Tensor([4,6])).to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=num_train_optimization_steps)\n",
    "\n",
    "#print(optimizer)\n",
    "#model.freeze_bert_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([param.requires_grad for param in model.bert.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/04/2019 21:46:48 - INFO - __main__ -   _____ Epoch 0 begin _____\n",
      "TRAIN: 100%|██████████| 2481/2481 [09:32<00:00,  3.22it/s]\n",
      "11/04/2019 21:56:21 - INFO - __main__ -   ***** Eval train results *****\n",
      "11/04/2019 21:56:21 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 21:56:21 - INFO - __main__ -     mcm = [[[15652    28]\n",
      "  [23957    44]]\n",
      "\n",
      " [[   44 23957]\n",
      "  [   28 15652]]]\n",
      "11/04/2019 21:56:21 - INFO - __main__ -     loss = 0.07767115867680766\n",
      "11/04/2019 21:56:21 - INFO - __main__ -     acc = 0.39555454751644364\n",
      "11/04/2019 21:56:39 - INFO - __main__ -   ***** Eval test results *****\n",
      "11/04/2019 21:56:39 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 21:56:39 - INFO - __main__ -     mcm = [[[1776    1]\n",
      "  [2633    0]]\n",
      "\n",
      " [[   0 2633]\n",
      "  [   1 1776]]]\n",
      "11/04/2019 21:56:39 - INFO - __main__ -     loss = 0.07555383591154535\n",
      "11/04/2019 21:56:39 - INFO - __main__ -     acc = 0.40272108843537413\n",
      "11/04/2019 21:56:39 - INFO - __main__ -   @@@@@ Save best model @@@@@\n",
      "11/04/2019 21:56:39 - INFO - __main__ -   file_path = /home/wy506wd/data/2c_wash/pytorch_model.bin\n",
      "/home/wy506wd/anaconda2/envs/env_wy/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BertForSequenceClassification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "11/04/2019 21:56:40 - INFO - __main__ -   _____ Epoch 1 begin _____\n",
      "TRAIN: 100%|██████████| 2481/2481 [09:49<00:00,  4.85it/s]\n",
      "11/04/2019 22:06:30 - INFO - __main__ -   ***** Eval train results *****\n",
      "11/04/2019 22:06:30 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 22:06:30 - INFO - __main__ -     mcm = [[[ 8031  7649]\n",
      "  [11190 12811]]\n",
      "\n",
      " [[12811 11190]\n",
      "  [ 7649  8031]]]\n",
      "11/04/2019 22:06:30 - INFO - __main__ -     loss = 0.04462944500457916\n",
      "11/04/2019 22:06:30 - INFO - __main__ -     acc = 0.5252387792646355\n",
      "11/04/2019 22:06:48 - INFO - __main__ -   ***** Eval test results *****\n",
      "11/04/2019 22:06:48 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 22:06:48 - INFO - __main__ -     mcm = [[[   0 1777]\n",
      "  [   0 2633]]\n",
      "\n",
      " [[2633    0]\n",
      "  [1777    0]]]\n",
      "11/04/2019 22:06:48 - INFO - __main__ -     loss = 0.04307898193800531\n",
      "11/04/2019 22:06:48 - INFO - __main__ -     acc = 0.5970521541950113\n",
      "11/04/2019 22:06:48 - INFO - __main__ -   @@@@@ Save best model @@@@@\n",
      "11/04/2019 22:06:48 - INFO - __main__ -   file_path = /home/wy506wd/data/2c_wash/pytorch_model.bin\n",
      "11/04/2019 22:06:49 - INFO - __main__ -   _____ Epoch 2 begin _____\n",
      "TRAIN: 100%|██████████| 2481/2481 [09:50<00:00,  4.96it/s]\n",
      "11/04/2019 22:16:40 - INFO - __main__ -   ***** Eval train results *****\n",
      "11/04/2019 22:16:40 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 22:16:40 - INFO - __main__ -     mcm = [[[ 8684  6996]\n",
      "  [11459 12542]]\n",
      "\n",
      " [[12542 11459]\n",
      "  [ 6996  8684]]]\n",
      "11/04/2019 22:16:40 - INFO - __main__ -     loss = 0.04298319145242869\n",
      "11/04/2019 22:16:40 - INFO - __main__ -     acc = 0.5349159547390439\n",
      "11/04/2019 22:16:58 - INFO - __main__ -   ***** Eval test results *****\n",
      "11/04/2019 22:16:58 - INFO - __main__ -     task = 2c_wash\n",
      "11/04/2019 22:16:58 - INFO - __main__ -     mcm = [[[1538  239]\n",
      "  [2017  616]]\n",
      "\n",
      " [[ 616 2017]\n",
      "  [ 239 1538]]]\n",
      "11/04/2019 22:16:58 - INFO - __main__ -     loss = 0.04236662186462593\n",
      "11/04/2019 22:16:58 - INFO - __main__ -     acc = 0.4884353741496599\n",
      "11/04/2019 22:16:58 - INFO - __main__ -   _____ Train finish _____\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "for epoch in range(N_EPOCHS):\n",
    "    logger.info('_____ Epoch %s begin _____'% epoch)\n",
    "    res = train_func(sub_train_,model)\n",
    "    train_results.append(res)\n",
    "    \n",
    "    res = test(sub_test_,model)\n",
    "    test_results.append(res)\n",
    "    test_acc = res['acc']\n",
    "    if test_acc > best_acc:\n",
    "        logger.info(\"@@@@@ Save best model @@@@@\")\n",
    "        logger.info('file_path = %s'% output_model_file)\n",
    "        best_acc = test_acc\n",
    "        acc_idx = len(test_results)-1\n",
    "        torch.save(model, output_model_file)\n",
    "        #model.config.to_json_file(output_config_file)\n",
    "        #tokenizer.save_vocabulary(OUTPUT_DIR)\n",
    "logger.info(\"_____ Train finish _____\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "#%matplotlib notebook \n",
    "\n",
    "  \n",
    "def show_map(indicator):\n",
    "    x = np.arange(0,len(train_results)) \n",
    "    y_train = [res[indicator] for res in train_results]\n",
    "    y_test = [res[indicator] for res in test_results]\n",
    "    plt.title(f\"{indicator} trend\") \n",
    "    plt.xlabel(\"Epoch\") \n",
    "    plt.ylabel(indicator) \n",
    "    if indicator != 'loss':\n",
    "        plt.ylim((0,1.0))\n",
    "    plt.plot(x,y_train,color='red',label='train') \n",
    "    plt.plot(x,y_test,color='blue',label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_precision(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def get_recall(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def get_F1_score(np):\n",
    "    tn, fp, fn, tp = np\n",
    "    P_precision = tp/(tp+fp)\n",
    "    P_recall = tp/(tp+fn)\n",
    "    return 2*(P_precision*P_recall)/(P_precision+P_recall)\n",
    "\n",
    "eval_functions = {\n",
    "    'precision' : get_precision,\n",
    "    'recall' : get_recall,\n",
    "    'F1_score' : get_F1_score\n",
    "}\n",
    "def plt_class(k):\n",
    "    x = np.arange(0,len(train_results)) \n",
    "    for name, func in eval_functions.items():\n",
    "        y_train = [func(res['mcm'][k].ravel()) for res in train_results]\n",
    "        y_test = [func(res['mcm'][k].ravel()) for res in test_results]\n",
    "        plt.title(f\"CLASS {k} {name}\") \n",
    "        plt.xlabel(\"Epoch\") \n",
    "        plt.ylabel(f'{name}') \n",
    "        plt.ylim((0,1.0))\n",
    "        plt.plot(x,y_train,color='red',label='train') \n",
    "        plt.plot(x,y_test,color='blue',label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "# logger.info(acc_1class)\n",
    "show_map('acc')\n",
    "show_map('loss')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    plt_class(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "acc = 53.1%\n",
      "class 0 Precision = 55.8%\n",
      "class 1 Precision = 51.6%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print('true_label')\n",
    "# _, _, o = dev_dataset\n",
    "# b = pd.DataFrame(o.cpu().numpy())\n",
    "# print(b[0].value_counts(sort=False, normalize=True).sort_index())\n",
    "\n",
    "# print('model_preds')\n",
    "# a = pd.DataFrame(preds)\n",
    "# print(a[0].value_counts(sort=False,normalize=True).sort_index())\n",
    "\n",
    "# acc_idx\n",
    "idx = acc_idx\n",
    "print('dev')\n",
    "print('acc = %.3f'% test_results[idx]['acc'])\n",
    "for i in range(2):\n",
    "    print('class %s Precision = %.3f'%(i,get_precision(test_results[idx]['mcm'][i].ravel())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = 'unfreeeze_summary'\n",
    "assert False\n",
    "\n",
    "# save model\n",
    "# model_to_save = model  # Only save the model it-self\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "# output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "# output_config_file = os.path.join(OUTPUT_DIR, CONFIG_NAME)\n",
    "\n",
    "torch.save(model, os.path.join(OUTPUT_DIR, f'{savename}.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "swing_model = torch.load(output_model_file)\n",
    "#swing_model = torch.load(os.path.join(OUTPUT_DIR, '8E_W.bin'))\n",
    "swing_model.to(device)\n",
    "\n",
    "# # test model\n",
    "# valid_loss, valid_acc = test(sub_valid_, model)\n",
    "# print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tsv_2_dataset():\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    logger.info('Load pre-trained model tokenizer (vocabulary)')\n",
    "    tokenizer = BertTokenizer.from_pretrained(f'/home/wy506wd/download/{BERT_MODEL}/')\n",
    "    processor = MultiClassificationProcessor()\n",
    "\n",
    "    # convert examples 2 features\n",
    "    logger.info('convert examples 2 features')\n",
    "    dev_examples = processor.get_train_examples(data_dir)\n",
    "    dev_examples_len = len(dev_examples)\n",
    "    label_list = processor.get_labels() # [0, 1] for binary classification\n",
    "    num_labels = len(label_list)\n",
    "    dev_features = convert_examples_to_features(dev_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in dev_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in dev_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in dev_features], dtype=torch.long)\n",
    "\n",
    "    if OUTPUT_MODE == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in dev_features], dtype=torch.long)\n",
    "    elif OUTPUT_MODE == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in dev_features], dtype=torch.float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def old_predict(model):\n",
    "#     all_predict_ids = []\n",
    "#     #model.cpu()\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     all_input_ids1 = all_input_ids.to(device)\n",
    "#     all_segment_ids1 = all_segment_ids.to(device)\n",
    "#     all_input_mask1 = all_input_mask.to(device)\n",
    "#     all_label_ids1 = all_label_ids.to(device)\n",
    "#     #print(all_input_ids1.is_cuda)\n",
    "#     with torch.no_grad():\n",
    "#         output = model(all_input_ids1, all_segment_ids1, all_input_mask1, labels=None)\n",
    "#     return output.argmax(1)\n",
    "\n",
    "def predict(data_, model):\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    sampler = SequentialSampler(data_)\n",
    "    data = DataLoader(data_, sampler=sampler, batch_size=TRAIN_BATCH_SIZE)\n",
    "    for batch in tqdm(data):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "            \n",
    "        preds += output.argmax(1).cpu().numpy().tolist()\n",
    "    return preds\n",
    "\n",
    "train_dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "all_text_a = [e.text_a for e in train_examples]\n",
    "all_predict_ids = predict(train_dataset,model)\n",
    "ans = pd.DataFrame({'text':all_text_a, 'label':all_label_ids, 'predict': all_predict_ids})\n",
    "ans\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ans['label'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "import collections\n",
    "\n",
    "NShortSegment = JClass(\"com.hankcs.hanlp.seg.NShort.NShortSegment\")\n",
    "ViterbiSegment = JClass(\"com.hankcs.hanlp.seg.Viterbi.ViterbiSegment\")\n",
    "\n",
    "nshort_segment = NShortSegment().enableCustomDictionary(False).enablePlaceRecognize(\n",
    "    True).enableOrganizationRecognize(True)\n",
    "shortest_segment = ViterbiSegment().enableCustomDictionary(\n",
    "    False).enablePlaceRecognize(True).enableOrganizationRecognize(True)\n",
    "\n",
    "# a = nshort_segment.seg(sentence)\n",
    "# print([i.word for i in a])\n",
    "\n",
    "CoreStopWordDictionary = JClass(\"com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary\")\n",
    "# CoreStopWordDictionary.apply(a)\n",
    "# print([i.word for i in a])\n",
    "\n",
    "# counts = [collections.Counter() for _ in range(2)]\n",
    "counts = collections.Counter()\n",
    "\n",
    "def count_words(text):\n",
    "    words = nshort_segment.seg(text)\n",
    "    CoreStopWordDictionary.apply(words)\n",
    "    word_list = [w.word for w in words]\n",
    "    counts.update(word_list)\n",
    "    return word_list\n",
    "\n",
    "topN = 128\n",
    "\n",
    "#ans.apply(count_words)\n",
    "ans['text'].map(count_words)\n",
    "print(len(counts))\n",
    "total_counts = counts\n",
    "common_words = {i[0] for i in counts.most_common(topN)}\n",
    "print('common_words:')\n",
    "print(common_words)\n",
    "predict_words = []\n",
    "for i in range(2):\n",
    "    counts = collections.Counter()\n",
    "    ans[ans['predict']==i]['text'].map(count_words)\n",
    "    predict_words.append({i[0] for i in counts.most_common(topN)})\n",
    "    print(f'predict = {i}')\n",
    "    print(predict_words[i] - common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross\n",
    "print(predict_words[0] - predict_words[1])\n",
    "print(predict_words[1] - predict_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = {i[0] for i in counts.most_common(64)}\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "print(counts.most_common(120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = ans[ans['label']!=ans['predict']]\n",
    "wrong.to_csv(os.path.join(data_dir, f\"{savename}_wrong.tsv\"),sep='\\t')\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swing = ans[ans['predict']==1]\n",
    "swing.to_csv(os.path.join(data_dir, f\"{savename}_swing.tsv\"),sep='\\t')\n",
    "swing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv(os.path.join(data_dir, f\"{savename}_predict.tsv\"),sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels = [torch.from_numpy(np.array(1))]\n",
    "list_of_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_wy",
   "language": "python",
   "name": "env_wy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
